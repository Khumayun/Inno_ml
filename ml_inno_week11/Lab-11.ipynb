{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-11: Ensemble Learning\n",
    "\n",
    "```\n",
    "- Machine Learning, Innopolis University (Fall semester 2021)\n",
    "- Professor: Adil Khan\n",
    "- Teaching Assistant: Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "\n",
    "```\n",
    "In this lab, you will practice Ensemble learning\n",
    "\n",
    "Lab Plan : Ensemble learning\n",
    "1. Bagging \n",
    "2. Random Forests\n",
    "3. AdaBoost\n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4G2NXb2yznzd"
   },
   "source": [
    "## 1. Ensemble learning\n",
    "\n",
    "\n",
    "Why ensemble learning? How does it help?\n",
    "\n",
    "\n",
    "We will explore ensemble learning on the example of decision trees - we will see how ensembles can improve classification accuracy.\n",
    "\n",
    "Let's start from uploading MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "colab_type": "code",
    "id": "9sI82NDtzoSP",
    "outputId": "a3162ec7-b6cb-4258-dcc6-86eca1157e99"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "\n",
    "\n",
    "plt.figure(1, figsize=(3, 3))\n",
    "plt.imshow(X[0].reshape((8,8)), cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(f\"label is {y[0]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcJSouftKwUC"
   },
   "source": [
    "## Recap : Single decision tree\n",
    "\n",
    "First, we train a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aqrQbHVDKw9F",
    "outputId": "c7ebcbc8-41cd-49c3-8d44-5bed82c74265"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "pred = tree.predict(X_test)\n",
    "tree_score = accuracy_score(y_test, pred)\n",
    "print(\"Single tree accuracy:\", tree_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qgAj4l5GLKuG"
   },
   "source": [
    "Note the accuracy - it is around **0.85**.\n",
    "\n",
    "## 1. Bagging\n",
    "\n",
    "\n",
    "What is decreased by bagging? Variance or bias? How? \n",
    "\n",
    "Now let's improve it a bit by the means of bagging. We train a hundred of independent classifiers and make a prediction by majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "xWQTCYrGLLMM",
    "outputId": "c762f1dd-631c-40d3-f1cd-d45855890388"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "n_trees = 100\n",
    "\n",
    "classifiers = []\n",
    "for i in range(n_trees):\n",
    "    # TODO: train a new classifier and append it to the list\n",
    "    pass\n",
    "\n",
    "\n",
    "# here we will store predictions for all samples and all base classifiers\n",
    "base_pred = np.zeros((X_test.shape[0], n_trees), dtype=\"int\")\n",
    "for i in range(n_trees):\n",
    "    # TODO: obtain the predictions from each tree\n",
    "    base_pred[:,i] = None\n",
    "\n",
    "print(base_pred)\n",
    "\n",
    "# aggregate predictions by majority voting\n",
    "pred = mode(base_pred, axis=1)[0].ravel()\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Bagging accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODOPvJJ2bKPh"
   },
   "source": [
    "Now the accuracy grew up to **0.88**. You can see that our classifiers return very similar results. By the way, why the base classifiers are not identical at all?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random forest\n",
    "\n",
    "Compared to simple bagging we've just implemented, random forest can show better results because base classifiers are much less correlated.\n",
    "\n",
    "At first, let's implement bootstrap sampling.\n",
    "\n",
    "### Task : implement bootstrap sampling (use numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "C9d1ElFpE48c",
    "outputId": "9c6fc189-e7fa-4b2a-a5db-41403aab23c4"
   },
   "outputs": [],
   "source": [
    "def bootstrap(X, y):\n",
    "    # TODO: generate bootstrap indices and return data according to them\n",
    "    X_bs = None\n",
    "    y_bs = None\n",
    "    return X_bs, y_bs\n",
    "\n",
    "\n",
    "# this is a test, will work if you are using np.random.randint() for indices generation\n",
    "np.random.seed(0)\n",
    "a = np.array(range(12)).reshape(4,3)\n",
    "b = np.array(range(4))\n",
    "bootstrap(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NxpCck94Y73A"
   },
   "source": [
    "You should get\n",
    "\n",
    "(array([[ 0,  1,  2], <br>\n",
    "&emsp;&emsp;&emsp;[ 9, 10, 11], <br>\n",
    "&emsp;&emsp;&emsp;[ 3,  4,  5], <br>\n",
    "&emsp;&emsp;&emsp;[ 0,  1,  2]]), <br>\n",
    "array([0, 3, 1, 0]))\n",
    "       \n",
    "## 2.1 Random forest from scratch\n",
    "\n",
    "Now let's build a set of decision trees, each of them is trained on a bootstrap sampling from X and $\\sqrt d$ features.\n",
    "\n",
    "### Task : Build Random forest from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HxIhmI_H5jnI",
    "outputId": "b2e3e694-f1a1-4513-b281-f7acac8febe1"
   },
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "for i in range(n_trees):\n",
    "    # TODO: train a new tree on sqrt(n_features) and bootstrapped data, append it to the list\n",
    "    pass\n",
    "\n",
    "base_pred = np.zeros((n_trees, X_test.shape[0]), dtype=\"int\")\n",
    "for i in range(n_trees):\n",
    "    base_pred[i,:] = classifiers[i].predict(X_test)\n",
    "\n",
    "pred = mode(base_pred, axis=0)[0].ravel()\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Random forest accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObGBbQfrE5jM"
   },
   "source": [
    "And now we got **0.97** accuracy, which is a significant improvement! Now you can see why it is so important to have diverse classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qp4WWdYezpHy"
   },
   "source": [
    "## 3. Boosting\n",
    "\n",
    "How does boosting work? \n",
    "\n",
    "For simplicity let's make a binary classification problem out of the original problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JBbO7p-aNk69"
   },
   "outputs": [],
   "source": [
    "y_train_b = (y_train == 2 ) * 2 - 1\n",
    "y_test_b = (y_test == 2 ) * 2 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "talvfivTQitE"
   },
   "source": [
    "Now let's train a boosting model.\n",
    "\n",
    "We will have sample weights and tree weights. Initially all sample weights are equal. After that we will increase weight for complicated samples.\n",
    "\n",
    "Tree weight $w$ is computed using weighted error or $1 - accuracy$\n",
    "\n",
    "$w_t = \\frac12 log(\\frac{1-weighted\\_error_t}{weighted\\_error_t})$ for each base classifier.\n",
    "\n",
    "For correct samples weights will be decreased $e^w$ times, and for incorrect classified samples increased  $e^w$ times. After this changes we normalize weights.\n",
    "\n",
    "## 3.1 Boosting from Scratch \n",
    "\n",
    "Tasks: \n",
    "1. initialize sample weights\n",
    "1. caclulate tree weight\n",
    "1. update sample weights\n",
    "1. normalize the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mVEBRi4pEwte"
   },
   "outputs": [],
   "source": [
    "n_trees = 10\n",
    "tree_weights = np.zeros(n_trees)\n",
    "classifiers = []\n",
    "train_samples = X_train.shape[0]\n",
    "# TODO : initialize sample weights\n",
    "sample_weights = None\n",
    "for i in range(n_trees):\n",
    "    clf = DecisionTreeClassifier(min_samples_leaf=3)\n",
    "    clf.fit(X_train, y_train_b, sample_weight=sample_weights)\n",
    "    pred = clf.predict(X_train)\n",
    "    acc = accuracy_score(y_train_b, pred, sample_weight=sample_weights)\n",
    "    # TODO: caclulate tree weight\n",
    "    w = None\n",
    "    tree_weights[i] = None\n",
    "    classifiers.append(clf)\n",
    "    # update sample weights\n",
    "    for j in range(train_samples):\n",
    "        if pred[j] != y[j]:\n",
    "            # in case of a misclassification\n",
    "            pass\n",
    "        else:\n",
    "            # in case when the prediction is correct\n",
    "            pass\n",
    "    # normalize the weights\n",
    "    sample_weights = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJ43LIorSXbs"
   },
   "source": [
    "Use trees voting to calculate final predictions. Since we have a binary classification, the prediction will be calculated as follows:\n",
    "\n",
    "$\\hat{y} = sign(\\sum_{t=1}^{T}(w_t f_t(x)))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PpJKjCDdzpmt",
    "outputId": "4d7db356-1a10-413f-c432-1877b3f46b72"
   },
   "outputs": [],
   "source": [
    "n_test = X_test.shape[0]\n",
    "\n",
    "pred = np.zeros(n_test)\n",
    "# TODO: caclulate predictions\n",
    "\n",
    "acc = accuracy_score(y_test_b, pred)\n",
    "print(\"Boosting accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E7mv1TfwSahW"
   },
   "source": [
    "The resulting accuracy is **0.97**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional : Compare your implementation with sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab7_answers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
