{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab-10: Decision Trees\n",
    "\n",
    "```\n",
    "- Machine Learning, Innopolis University (Fall semester 2021)\n",
    "- Professor: Adil Khan\n",
    "- Teaching Assistant: Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "\n",
    "```\n",
    "In this lab, you will practice Decision Trees\n",
    "\n",
    "Lab Plan\n",
    "1. Decision Trees:\n",
    "    - Sklearn    \n",
    "    - Visualize DT\n",
    "    - Gini, entropy\n",
    "    - Information Gain\n",
    "    - Regression Trees\n",
    "    \n",
    "2. Comparison between models\n",
    "    - Linearly Seperable\n",
    "    - Boolean XOR\n",
    "    - Moons\n",
    "    - Circles\n",
    "\n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d1bAS7mykOeg"
   },
   "source": [
    "## 1. Decision Trees\n",
    "### Recap\n",
    "\n",
    "1.  What do leaves and branches represent in Decision trees?\n",
    "\n",
    "2. When do we stop splitting the tree?\n",
    "\n",
    "3. What is Entropy?\n",
    "\n",
    "4. What is Information Gain?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that we will work on is the same dataset that we saw before which is Iris image calssification.\n",
    "\n",
    "### Task Load iris dataset from sklearn and Separate to independent and dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "elXqNfsUo8CG",
    "outputId": "9b43b89c-8ab4-487a-9786-f9e2ec9b468e"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iris = None\n",
    "X = None\n",
    "y = None\n",
    "\n",
    "\n",
    "pd.DataFrame(iris.data,columns=iris.feature_names).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Define, Train & visualize Decision Tree Classifier\n",
    "\n",
    "### Task : Define Decision Tree Classifier from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "J1rT-E6LR5lI",
    "outputId": "533d0e45-9b8a-44cc-cd0d-946e045503b1"
   },
   "outputs": [],
   "source": [
    "decision_tree = None\n",
    "decision_tree = decision_tree.fit(X, y)\n",
    "r = export_text(decision_tree, feature_names=iris['feature_names'])\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fkpN6eYDzs-D"
   },
   "source": [
    "## Visualize decision tree with Graphviz\n",
    "\n",
    "### What is Graphviz? \n",
    "\n",
    "Now let's see what kind of trees can we build with Sklearn. Then visualize with graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 916
    },
    "colab_type": "code",
    "id": "Xyxvmt5DRIqc",
    "outputId": "20cadbd7-2b86-49a6-9ff4-a7b592962bca"
   },
   "outputs": [],
   "source": [
    "import graphviz \n",
    "def plot_tree(clf):\n",
    "    dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                       feature_names=iris.feature_names,  \n",
    "                       class_names=iris.target_names,  \n",
    "                       filled=True, rounded=True,  \n",
    "                       special_characters=True)  \n",
    "    graph = graphviz.Source(dot_data)  \n",
    "    return graph \n",
    "graph = plot_tree(decision_tree)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAWfzgYzz3FQ"
   },
   "source": [
    "let's use a smaller Decison Tree and try to recreate some of the values on it.\n",
    "\n",
    "Calculate the gini and entropy of the second branch (Petal width (cm) <= 1.75). \n",
    "you can hover over the branches to read their number.\n",
    "\n",
    "\n",
    "![alt text](https://miro.medium.com/max/565/1*M15RZMSk8nGEyOnD8haF-A.png)\n",
    "![alt text](https://qph.fs.quoracdn.net/main-qimg-93d3fa675b807bc505ef905c828d6c6d)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "C34me0dgVAWF",
    "outputId": "74895f47-7316-4741-a6dd-68e4b9240a07"
   },
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier(criterion = \"entropy\", max_depth=3)\n",
    "decision_tree = decision_tree.fit(X, y)\n",
    "plot_tree(decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Impurity Criterion\n",
    "\n",
    "Let's calculate the gini index and entropy which we will use in information gain calculations\n",
    "\n",
    "### Task : Implement gini index calculating function and entropy calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SfTguPMCoGEO",
    "outputId": "e591482e-ba63-47df-cf12-cb74b3b6f594"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement a gini index calculating function \n",
    "def gini(value):\n",
    "    #TODO : 2-3 lines of code\n",
    "    return None\n",
    "\n",
    "# TODO: Implement a entropy calculating function \n",
    "def entropy(value):\n",
    "    #TODO : 2-3 lines of code\n",
    "    return None\n",
    "\n",
    "\n",
    "value = np.array([50,50,0,10,0,10])\n",
    "print(\"gini: {:.03}, entropy: {:.04} \".format(gini(value),entropy(value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e7KLRj9cooBR",
    "outputId": "d81119c9-3dad-45dc-c3a4-750a5c23244e"
   },
   "outputs": [],
   "source": [
    "def information_gain(parent, children):\n",
    "    total = parent.sum()\n",
    "    s = entropy(parent)\n",
    "    for child in children:\n",
    "        s -= child.sum()/total * entropy(child)\n",
    "    return s\n",
    "parent = np.array([50,50,50])\n",
    "children = np.array([[50,0,0],[0,50,50]])\n",
    "ig = information_gain(parent, children)\n",
    "print(\"information gain: {:.03}\".format(ig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "slPIrYHmrIXP"
   },
   "source": [
    "## 1.3 Regression Trees\n",
    "\n",
    "Basic Concept:\n",
    "* Split the data as before.\n",
    "* To minimize the the variance of the leaf node values.\n",
    "* Take the mean as the prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "FtPR2mM_r9jU",
    "outputId": "96d6583f-7d74-4ce1-de4a-434c7b3e58a9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a random dataset\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.5 - rng.rand(16))\n",
    "\n",
    "# Fit regression model\n",
    "regr_1 = DecisionTreeRegressor(max_depth=2)\n",
    "regr_2 = DecisionTreeRegressor(max_depth=5)\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_1 = regr_1.predict(X_test)\n",
    "y_2 = regr_2.predict(X_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\",\n",
    "            c=\"darkorange\", label=\"data\")\n",
    "plt.plot(X_test, y_1, color=\"cornflowerblue\",\n",
    "         label=\"max_depth=2\", linewidth=2)\n",
    "plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fyZoAyaV4dT5"
   },
   "source": [
    "# 2. Comparison Between models\n",
    "Now that you know multiple types of classifiers:\n",
    "1. Linear\n",
    "  * Logistic Regression\n",
    "  * SVM-(with linear kernel)\n",
    "2. Non-Linear\n",
    "  * SVM-(with non linear kernel)\n",
    "  * KNN\n",
    "  * Decision Trees\n",
    "  * Gaussian Naive Bayes\n",
    "  \n",
    "Let's take a look how their decision boundries look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "colab_type": "code",
    "id": "skf4MS-iuhLm",
    "outputId": "870f60a1-92ea-4b97-f4b6-224b176380a2"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initializing Classifiers\n",
    "# TODO: Logistic Regression\n",
    "clf1 = None\n",
    "# TODO: Random Forest Classifier with 100 estimators\n",
    "clf2 = None \n",
    "#TODO: Gaussian Naive bayes classifier \n",
    "clf3 = None\n",
    "#TODO: Support vector machine classifier\n",
    "clf4 = None\n",
    "\n",
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "labels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM']\n",
    "for clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n",
    "                         labels,\n",
    "                         itertools.product([0, 1], repeat=2)):\n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    plt.title(lab)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "colab_type": "code",
    "id": "mAddv6EeIzLF",
    "outputId": "858febed-3f18-417f-c0c1-2e307a41377f"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "# Loading Plotting Utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initializing Classifiers\n",
    "clf1 = LogisticRegression(random_state=1, solver='lbfgs')\n",
    "clf2 = RandomForestClassifier(n_estimators=100,\n",
    "                              random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = SVC(gamma='auto')\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(-3, 3, 50),\n",
    "                     np.linspace(-3, 3, 50))\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.randn(300, 2)\n",
    "y = np.array(np.logical_xor(X[:, 0] > 0, X[:, 1] > 0),\n",
    "             dtype=int)\n",
    "\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "labels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM']\n",
    "for clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n",
    "                         labels,\n",
    "                         itertools.product([0, 1], repeat=2)):\n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    plt.title(lab)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "colab_type": "code",
    "id": "A5_7UTYUI3Hw",
    "outputId": "b2daf414-cddb-4864-ace8-86dc49dffac9"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=100, random_state=123)\n",
    "\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "labels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM']\n",
    "for clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n",
    "                         labels,\n",
    "                         itertools.product([0, 1], repeat=2)):\n",
    "\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    plt.title(lab)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "colab_type": "code",
    "id": "-MOspkViI2-B",
    "outputId": "3a8c745e-d408-4774-a0d0-8a2ec134f94a"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "X, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\n",
    "\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "labels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM']\n",
    "for clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n",
    "                         labels,\n",
    "                         itertools.product([0, 1], repeat=2)):\n",
    "\n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    plt.title(lab)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "students_week6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
