{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "metallic-hierarchy",
   "metadata": {},
   "source": [
    "# Assignment 2 Checkpoint\n",
    "\n",
    "```\n",
    "- Machine Learning, Innopolis University (Fall semester 2021)\n",
    "- Professor: Adil Khan\n",
    "- Teaching Assistant: Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "```\n",
    "Lab Plan \n",
    "1. Autoencoders \n",
    "2. Custom loss functions\n",
    "3. Data Preprocessing\n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-preservation",
   "metadata": {},
   "source": [
    "## 1. Autoencoders \n",
    "\n",
    "* Types of autoencoders\n",
    "* Applications of autoencoders\n",
    "* Autoencoders training procedure\n",
    "* Reparametrisation trick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-muslim",
   "metadata": {},
   "source": [
    "![caption](https://miro.medium.com/max/2400/1*Q5dogodt3wzKKktE0v3dMQ@2x.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-medline",
   "metadata": {},
   "source": [
    "## 1.1 Defining AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_dim):\n",
    "        super(autoencoder, self).__init__()\n",
    "        # Step 1 : Define the encoder \n",
    "        # Step 2 : Define the decoder\n",
    "        # Step 3 : Initialize the weights (optional)\n",
    "        self.encoder = nn.Sequential(\n",
    "          nn.Linear(input_size, input_size//2),\n",
    "          nn.ReLU(True),\n",
    "          nn.Linear(input_size//2, input_size//3),\n",
    "          nn.Linear(input_size//3, input_size//4),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(input_size//4, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "          nn.Linear(latent_dim, input_size//4),\n",
    "          nn.ReLU(True),\n",
    "          nn.Linear(input_size//4, input_size//3),\n",
    "          nn.Linear(input_size//3, input_size//2),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(input_size//2, input_size)\n",
    "        )\n",
    "        self.encoder.apply(self.__init_weights)\n",
    "        self.decoder.apply(self.__init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Step 1: Pass the input through encoder to get latent representation\n",
    "        # Step 2: Take latent representation and pass through decoder\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def encode(self,input):\n",
    "        #Step 1: Pass the input through the encoder to get latent representation\n",
    "        return self.encoder(input)\n",
    "\n",
    "    def __init_weights(self,m):\n",
    "        #Init the weights (optional)\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-olympus",
   "metadata": {},
   "source": [
    "## 1.2 Training AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 100\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "sample = torch.randn((batchSize,1,64))\n",
    "AE = autoencoder(64,5).to(device)\n",
    "print(AE)\n",
    "# print(summary(AE,input_size=(1, 64)))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(AE.parameters(),lr=learning_rate)\n",
    "\n",
    "#Create a random dataset\n",
    "data_loader = DataLoader(TensorDataset(torch.randn((1000,1,64))),batch_size=32,shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for X in data_loader:\n",
    "        X = X[0].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # forward\n",
    "        output = AE(X)\n",
    "        loss = criterion(output, X)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # log\n",
    "        print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-terry",
   "metadata": {},
   "source": [
    "## 1.3 CNN AE\n",
    "\n",
    "Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        ## encoder layers ##\n",
    "        # conv layer (depth from 1 --> 16), 3x3 kernels\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)  \n",
    "        # conv layer (depth from 16 --> 4), 3x3 kernels\n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
    "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        ## decoder layers ##\n",
    "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
    "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(16, 1, 2, stride=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## encode ##\n",
    "        # add hidden layers with relu activation function\n",
    "        # and maxpooling after\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        # add second hidden layer\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)  # compressed representation\n",
    "        \n",
    "        ## decode ##\n",
    "        # add transpose conv layers, with relu activation function\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        # output layer (with sigmoid for scaling from 0 to 1)\n",
    "        x = F.sigmoid(self.t_conv2(x))\n",
    "                \n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = ConvAutoencoder()\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
