{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 13 : Generative Adversarial Networks\n",
    "```\n",
    "- Machine Learning, Innopolis University \n",
    "- Professor: Adil Khan \n",
    "- Teaching Assistant: Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "\n",
    "```\n",
    "Lab Plan\n",
    "    1. Vanila GAN achitecture \n",
    "    2. GAN training procedure\n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vannila Generative adversarial network (GAN)\n",
    "\n",
    "![caption](https://www.researchgate.net/profile/Zhaoqing-Pan/publication/331756737/figure/fig1/AS:736526694621184@1552613056409/The-architecture-of-generative-adversarial-networks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset \n",
    "\n",
    "For this lesson we will use SVHN dataset which readily available in `torchvision` and we will do minimal transformation operations \n",
    "\n",
    "### Task : Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "99446fed60054507a3816607c57ce5b6",
      "f43e4af85ed846be8d54f7ce0951ff1f",
      "c6025291f61748fc9513e56c6d6c9fa0",
      "8bfcb58458754f129976fc7ca4cbed03",
      "79489983337548eb97d6e1c21e24b532",
      "d07ad6972e9049fd8e5ef3d5cd5f3f90",
      "f67eb6d205424dbaa12a510694036317",
      "d131b597604249fbaefa428aae097039"
     ]
    },
    "colab_type": "code",
    "id": "AgVjuBIo9YcY",
    "outputId": "8cf17d50-317f-4608-f12b-c85aa90f0aff"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def normalize(data_tensor):\n",
    "    '''re-scale image values to [-1, 1]'''\n",
    "    pass\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: normalize(x))])\n",
    "\n",
    "# SVHN training datasets\n",
    "svhn_train = datasets.SVHN(root='data/', split='train', download=True, transform=transform)\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 0\n",
    "\n",
    "# build DataLoaders for SVHN dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=svhn_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Generator & Discriminator Definition\n",
    "\n",
    "There are a couple of ways to increase the input of the generator (*z*) to the desired output size.\n",
    "1. Number of neurones\n",
    "2. Transposed Convolutions `torch.nn.ConvTranspose2d` [More info](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html)\n",
    "\n",
    "### TASK : Define the generator and discriminator network using the architectures specified below\n",
    "\n",
    "#### Discriminator : <br>\n",
    "```\n",
    "1. conv layer 1 -> output channels 32, kernel size 4x4, stride 2x2\n",
    "2. conv layer 2 -> output channels 64, kernel size 4x4, stride 2x2\n",
    "3. Add batch normalization & Leaky ReLU activation \n",
    "4. conv layer 3 -> output channels 1, kernel_size 4x4, stride 1x1\n",
    "5. Add batch normalization\n",
    "6. Flatten layer\n",
    "7. Output layer\n",
    "```\n",
    "\n",
    "#### Generator : <br>\n",
    "```\n",
    "1. Transpose 2d layer -> output channels 6, kernel size 4x4, stride 2x2, padding 1\n",
    "2. Add batch normalization & Leaky Tanh activation \n",
    "3. Transpose 2d layer -> output channels 3, kernel size 4x4, stride 2x2, padding 1 \n",
    "4. Batch normalization & Leaky Tanh activation \n",
    "5. Transpose 2d layer -> output channels 3, kernel size 4x4, stride 2x2, padding 1 \n",
    "6. Tanh activation \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_dim=32):\n",
    "        super(Discriminator, self).__init__()\n",
    "        #TODO: \n",
    "        self.model = nn.Sequential(...)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Step 1: pass the input (real or fake samples) through all hidden layers\n",
    "        pass\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_size, conv_dim=32):\n",
    "        super(Generator, self).__init__()\n",
    "        # Step 1: Define the generator network architecture\n",
    "        # NOTE: the input is the random noise size and output is conv_dim i.e (3,32,32)\n",
    "        self.model = nn.Sequential(...) \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Step 1: pass the input which is random noise to generate the fake samples\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Set hyperparams and training parameters\n",
    "\n",
    "### Task : create discriminator and generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparams\n",
    "conv_dim = 32\n",
    "z_size = 100\n",
    "num_epochs = 50\n",
    "\n",
    "# TODO: define discriminator and generator and send it to device \n",
    "D = None\n",
    "G = None\n",
    "\n",
    "#print the models summary \n",
    "print(D)\n",
    "print()\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Define the loss function for D(x) and G(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def real_loss(D_out, smooth=False):\n",
    "    batch_size = D_out.size(0)\n",
    "    # label smoothing\n",
    "    if smooth:\n",
    "        # smooth, real labels\n",
    "        labels = torch.FloatTensor(batch_size).uniform_(0.9, 1).to(device)\n",
    "    else:\n",
    "        labels = torch.ones(batch_size).to(device) # real labels = 1     \n",
    "    \n",
    "    # binary cross entropy with logits loss\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss\n",
    "\n",
    "def fake_loss(D_out):\n",
    "    batch_size = D_out.size(0)\n",
    "    labels = torch.FloatTensor(batch_size).uniform_(0, 0.1).to(device) # fake labels approx 0\n",
    "    labels = labels.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss\n",
    "\n",
    "# params\n",
    "learning_rate = 0.0003\n",
    "beta1=0.5\n",
    "beta2=0.999 # default value\n",
    "\n",
    "# Create optimizers for the discriminator and generator\n",
    "d_optimizer = None\n",
    "g_optimizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 GAN training Loop\n",
    "\n",
    "### Task \n",
    "\n",
    "1. Implement GAN training procedure\n",
    "1. (optional) : Add TensorBoard to monitor the generator and discriminator loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# keep track of loss and generated, \"fake\" samples\n",
    "losses = []\n",
    "\n",
    "print_every = 2\n",
    "\n",
    "# Get some fixed data for sampling. These are images that are held\n",
    "# constant throughout training, and allow us to inspect the model's performance\n",
    "sample_size=16\n",
    "fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
    "fixed_z = torch.from_numpy(fixed_z).float()\n",
    "\n",
    "# train the network\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch_i, (real_images, _) in enumerate(train_loader):\n",
    "                \n",
    "        batch_size = real_images.size(0)\n",
    "        \n",
    "        \n",
    "        # TODO: TRAIN THE DISCRIMINATOR\n",
    "        # Step 1: Zero gradients (zero_grad)\n",
    "        # Step 2: Train with real images\n",
    "        # Step 3: Compute the discriminator losses on real images \n",
    "        \n",
    "        D_real = None\n",
    "        d_real_loss = real_loss(D_real)\n",
    "        \n",
    "        # Step 4: Train with fake images\n",
    "        # Step 5: Generate fake images and move x to GPU, if available\n",
    "        # Step 6: Compute the discriminator losses on fake images \n",
    "        # Step 7: add up loss and perform backprop\n",
    "        \n",
    "        fake_images = None     \n",
    "        \n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        \n",
    "        #TODO: TRAIN THE GENERATOR (Train with fake images and flipped labels)\n",
    "        g_optimizer.zero_grad()\n",
    "        \n",
    "        # Step 1: Zero gradients  \n",
    "        # Step 2: Generate fake images from random noise (z)\n",
    "        # Step 3: Compute the discriminator losses on fake images using flipped labels!\n",
    "        # Step 4: Perform backprop and take optimizer step\n",
    "\n",
    "    # TODO: Print some loss stats\n",
    "    if epoch % print_every == 0:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aQsNSLUy-sbV"
   },
   "source": [
    "Keep in mind:\n",
    "\n",
    "1. Always use a learning rate for discriminator higher than the generator.\n",
    "\n",
    "2. Keep training even if you see that the losses are going up.\n",
    "\n",
    "3. There are many variations with different loss functions which are worth exploring.\n",
    "\n",
    "4. If you get mode collapse, lower the learning rates.\n",
    "\n",
    "5. Adding noise to the training data helps make the model more stable.\n",
    "\n",
    "6. Label Smoothing: instead of making the labels as 1 make it 0.9 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_abUCWlX8geC"
   },
   "source": [
    "## References\n",
    "\n",
    "1. [Deep Convolutional Generative Adversarial Network](https://www.tensorflow.org/tutorials/generative/dcgan)\n",
    "\n",
    "1. [Generative adversarial networks: What GANs are and how they’ve evolved](https://venturebeat.com/2019/12/26/gan-generative-adversarial-network-explainer-ai-machine-learning/)\n",
    "\n",
    "1. [Generative Adversarial Nets](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)\n",
    "\n",
    "1. [GANs by google](https://developers.google.com/machine-learning/gan)\n",
    "\n",
    "1. [A Gentle Introduction to Generative Adversarial Networks (GANs)](https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/)\n",
    "\n",
    "1. [A Beginner's Guide to Generative Adversarial Networks (GANs)](https://pathmind.com/wiki/generative-adversarial-network-gan)\n",
    "\n",
    "1. [Understanding Generative Adversarial Networks (GANs)](https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29)\n",
    "\n",
    "1. [Deep Learning (PyTorch)](https://github.com/udacity/deep-learning-v2-pytorch)\n",
    "\n",
    "1. [10 Lessons I Learned Training GANs for one Year](https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AML_Lab9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "79489983337548eb97d6e1c21e24b532": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8bfcb58458754f129976fc7ca4cbed03": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d131b597604249fbaefa428aae097039",
      "placeholder": "​",
      "style": "IPY_MODEL_f67eb6d205424dbaa12a510694036317",
      "value": "182042624it [00:05, 30969434.06it/s]"
     }
    },
    "99446fed60054507a3816607c57ce5b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c6025291f61748fc9513e56c6d6c9fa0",
       "IPY_MODEL_8bfcb58458754f129976fc7ca4cbed03"
      ],
      "layout": "IPY_MODEL_f43e4af85ed846be8d54f7ce0951ff1f"
     }
    },
    "c6025291f61748fc9513e56c6d6c9fa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d07ad6972e9049fd8e5ef3d5cd5f3f90",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_79489983337548eb97d6e1c21e24b532",
      "value": 1
     }
    },
    "d07ad6972e9049fd8e5ef3d5cd5f3f90": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d131b597604249fbaefa428aae097039": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f43e4af85ed846be8d54f7ce0951ff1f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f67eb6d205424dbaa12a510694036317": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
